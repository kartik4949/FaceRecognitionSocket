# -*- coding: utf-8 -*-
"""face recognition final v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iC4v0vUvM9gMhSRIv7pz6ZeI_WMt2RXL
"""
'''
import cv2
import os
from mtcnn.mtcnn import MTCNN
from tqdm import tqdm
class load_faces:
    def __init__(self,path):
        self.path  = path
        self.knownfaces = []
        self.label = []
    def loadarray(self):
        for o, directory, files in os.walk(self.path):
            for filename in files:
                temp = cv2.imread(o+ '/'+ filename)
                self.knownfaces.append(cv2.cvtColor(temp,cv2.COLOR_BGR2RGB))
                self.label.append(o.split('/')[-1])
                
l = load_faces('Known_Faces/')
#l.loadarray()
#print(l.label)



##ABOVE CODE FOR LOADING FACES FROM MTCNN OUPUT

"""# EXTRACTING FACES USING MTCNN"""

import matplotlib.pyplot as plt

class Face_Extractor:
    def __init__(self,array_faces=None):
        self.array_faces = array_faces
        self.results = []
        self.final = []
    def outputbounding_box(self,result):
        x1, y1, width, height = result
        x1, y1 = abs(x1), abs(y1)
        x2, y2 = x1 + width, y1 + height
        return (x1,y1),(x2,y2)

    
    def output_result(self):
        face_extractor = MTCNN()
        result = [face_extractor.detect_faces(face) for face in self.array_faces]
        for i in result:
            if(len(i)>0):
                self.results.append(i[0].get('box'))
       
            
    def getface_box(self):
        temp = []
        for i in tqdm(range(0,len(self.results))):
        
            (x1,y1),(x2,y2) = self.outputbounding_box(self.results[i])
            
            temp.append(cv2.resize(self.array_faces[i][y1:y2,x1:x2], (160,160), interpolation = cv2.INTER_AREA))
        return temp

from functools import wraps
import time
def decorate_getTime(func):
    @wraps(func)
    def wrapper(*args,**kwargs):
        for key, value in kwargs.items(): 
            count = len(value)
        start = time.time()
        
        result = func(*args,**kwargs)
        
        print('Execution Time per image:%.3f'%((time.time() - start)/count))
        return result
    return wrapper
        
        
@decorate_getTime
def generate_faces(knownfaces = []):
    f = Face_Extractor(array_faces = knownfaces)
    f.output_result()
    result = f.getface_box()
    return result

length = len(l.knownfaces[:5])
result  = generate_faces(knownfaces = l.knownfaces )

from keras.models import load_model
model = load_model('FaceNet/facenet_keras.h5')
print(model.outputs)

import numpy as np


def generate_faces_test(knownfaces = []):
    f = Face_Extractor(array_faces = knownfaces)
    f.output_result()
    result = []
    resulttupple = []
    if(len(f.results))>0:
        resulttupple = f.outputbounding_box(f.results[0])
        result = f.getface_box()
    return result,resulttupple

result,resulttupple = generate_faces_test([img])

resulttupple[0]

embeddings = model.predict([result])

xtest = ie.fit_transform(embeddings)

result = modelsvc.predict(xtest)

resultlabel = le.inverse_transform(result)

str(resultlabel[0])

img = cv2.rectangle(img,resulttupple[0],resulttupple[1],(0,255,0),1)
font = cv2.FONT_HERSHEY_SIMPLEX
img = cv2.putText(img,str(resultlabel[0]),resulttupple[0], font, 2,(255,255,255),1,cv2.LINE_AA)

def identifyFaces(img):
    resultlabel = ""
    result,resulttupple = generate_faces_test([img])
    if(len(result)>0):
        embeddings = model.predict([result])
        xtest = ie.fit_transform(embeddings)
        result = modelsvc.predict(xtest)
        resultlabel = le.inverse_transform(result)
    return resultlabel,resulttupple

r,t  = identifyFaces(cv2.cvtColor(cv2.imread('Testvideo/capture.jpg'),cv2.COLOR_BGR2RGB))

import cv2
import os
from mtcnn.mtcnn import MTCNN
import matplotlib.pyplot as plt
def detectFace(img,face_extractor):
    result = face_extractor.detect_faces(img)
    if(len(result)>0):
        temp = result[0].get('box')
        x1, y1 = abs(temp[0]), abs(temp[1])
        x2, y2 = x1 + temp[2], y1 + temp[3]
        tempimg = img.copy()
        tempimg =  cv2.resize(tempimg[y1:y2,x1:x2], (160,160), interpolation = cv2.INTER_AREA)
        embeddings = model.predict(np.reshape(tempimg,(1,160,160,3)))
        result = modelsvc.predict(embeddings)
        resultlabel = le.inverse_transform(result)
        img = cv2.putText(img,str(resultlabel[0]),(x1,y1),font, 2,(255,255,255),1,cv2.LINE_AA)
        img = cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),1)
    return img

import numpy as np
import cv2
cap = cv2.VideoCapture('Testvideo/test.mp4')
count = 0
font = cv2.FONT_HERSHEY_SIMPLEX
face_extractor = MTCNN()
while(True):
    ret, frame = cap.read()
    frame = detectFace(frame,face_extractor)
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

import numpy as np
import cv2
cap = cv2.VideoCapture('Testvideo/test.mp4')
count = 0
font = cv2.FONT_HERSHEY_SIMPLEX
while(True):
    ret, frame = cap.read()
    resultlabel , resulttupple = identifyFaces(frame)
    if(len(resulttupple))>0:
        frame = cv2.rectangle(frame,resulttupple[0],resulttupple[1],(0,255,0),1)
        frame = cv2.putText(frame,str(resultlabel[0]),resulttupple[0], font, 2,(255,255,255),1,cv2.LINE_AA)
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

cv2.imshow('image',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
'''

"""# AUTO INSTALLING MRCNN"""

import subprocess
import sys

#def install(package):
#    subprocess.call([sys.executable, "-m", "pip", "install", package])

#install('Pickle')

##MAIN CODE STARTS HERE USING KNN AND CAFFE OPENCV MODEL##


#####################################################################################################################################################################################
from keras.models import Sequential
import time
import math
from keras.optimizers import Adam
from keras.layers import Dense
from keras.layers import Conv2D,Input
from keras.layers import Flatten
from keras.layers import Dropout
from keras.models import Sequential
from keras.optimizers import Adam
from keras.layers import Dense
from keras.layers import Conv2D
from keras.layers import Flatten
from keras.layers import Dropout
from keras.activations import relu
from keras.layers import LeakyReLU
from keras.models import Model
from keras.layers import UpSampling2D
from keras.layers import BatchNormalization
from keras.layers import MaxPooling2D
from keras.layers import Permute
from keras.layers import Add
from keras.layers import Lambda
from keras.utils.vis_utils import plot_model
from keras.layers import dot,Activation,Reshape
import keras.backend as K
import itertools
import random

class Face_Extractor:
    def __init__(self,array_faces=None):
        self.array_faces = array_faces
        self.results = []
        self.final = []
    def outputbounding_box(self,result):
        x1, y1, width, height = result
        x1, y1 = abs(x1), abs(y1)
        x2, y2 = x1 + width, y1 + height
        return (x1,y1),(x2,y2)

    
    def output_result(self):
        face_extractor = MTCNN()
        result = [face_extractor.detect_faces(face) for face in self.array_faces]
        for i in result:
            if(len(i)>0):
                self.results.append(i[0].get('box'))
       
            
    def getface_box(self):
        temp = []
        for i in tqdm(range(0,len(self.results))):
            (x1,y1),(x2,y2) = self.outputbounding_box(self.results[i])
            temp.append(cv2.resize(self.array_faces[i][y1:y2,x1:x2], (160,160), interpolation = cv2.INTER_AREA))
        return temp

"""# DECORATER FOR TIME EVALUTION"""

from functools import wraps
import time
def decorate_getTime(func):
    @wraps(func)
    def wrapper(*args,**kwargs):
        for key, value in kwargs.items(): 
            count = len(value)
        start = time.time()
        result = func(*args,**kwargs)
        print('Execution Time per image:%.3f'%((time.time() - start)/count))
        return result
    return wrapper
        
        
@decorate_getTime
def generate_faces(knownfaces = []):
    f = Face_Extractor(array_faces = knownfaces)
    f.output_result()
    result = f.getface_box()
    return result

"""# SINGLETON DESIGN PATTERN USED FOR ENFORCING SINGLE INSTANCE CREATION"""

class Singleton(type):
    _instances = {}
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]

class metaclass_singleton(type):
    _class_instances = {}
    def __call__(cls , *args  , **kwargs):
        if cls not in cls._class_instances :
            print(cls)
            cls._class_instances[cls] = super(metaclass_singleton , cls).__call__( *args , **kwargs )
        return cls._class_instances[cls]

"""# TYPE ASSERTION USING SIGNATURE AND DECORATER"""

from inspect import signature
from functools import wraps



def typeassert(*ty_args, **ty_kwargs):
    def decorate(func):
        sig = signature(func)
        types = sig.bind_partial(*ty_args,**ty_kwargs).arguments
        @wraps(func)
        def wrapper(*args , **kwargs):
            values = sig.bind(*args, **kwargs)
            for n ,  v in values.arguments.items():
                if (n in types and 'self' not in n):
                    if not isinstance(v , types[n]):
                        raise TypeError(
                              'Argument {} must be {}'.format(n, types[n])
                             )
            return func(*args , **kwargs)
        return wrapper
    return decorate

"""# MAIN DETECTION CLASSES"""

import cv2
import os
from statistics import mean, median
from keras import optimizers
from mtcnn.mtcnn import MTCNN
from sklearn.svm import SVC
from keras.regularizers import l2
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
import cv2
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import Normalizer
from keras.models import load_model
import matplotlib.pyplot as plt
import pickle
from tqdm import tqdm
from os import listdir
from decimal import Decimal
import collections
def adjust_gamma(image, gamma=0.3):
    invGamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** invGamma) * 255
        for i in np.arange(0, 256)]).astype("uint8")
    return cv2.LUT(image, table)

def Image_Preprocessing_PipeLine(image):
    image= adjust_gamma(image)
    blur5 = cv2.GaussianBlur(image,(5,5),0)
    blur3 = cv2.GaussianBlur(image,(3,3),0)
    image = blur5 - blur3
    equ = cv2.equalizeHist(image)
    return equ





class DetectFaceBoundBoxes(metaclass = metaclass_singleton):
    def __init__(self,model):
        self.modelCaffe = model
        print('caffe model initialized....')

        
    
    def detectFace(self,image,h,w):
        a = None
        b = None
        confidence = None
        
        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.modelCaffe.setInput(blob)
        detections = self.modelCaffe.forward()
        max_confidence = 0
        for i in range(0, detections.shape[2]):

            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (startX, startY, endX, endY) = box.astype("int")
            #print(startX, startY, endX, endY)

            confidence = detections[0, 0, i, 2]
            #print('confidence',confidence)
            
            if(confidence > 0.4 ):
                if(i==0):
                    max_confidence = confidence
                if(confidence>=max_confidence):
                    max_confidence = confidence
                    a =  (startX, startY) 
                    b = (endX, endY)  
                    #print(max_confidence,a,b)

                
        #print(a,b,max_confidence)        
        return a, b , max_confidence
    def detect_multiFace(self,image,h,w):
        a = None
        b = None
        confidence = None
        face_list  = []
        confidence_list = []
        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.modelCaffe.setInput(blob)
        detections = self.modelCaffe.forward()
        max_confidence = 0
        for i in range(0, detections.shape[2]):

            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (startX, startY, endX, endY) = box.astype("int")
            #print(startX, startY, endX, endY)

            confidence = detections[0, 0, i, 2]
            #print('confidence',confidence)
            
            if(confidence > 0.4 ):
                a =  (startX, startY) 
                b = (endX, endY)
                confidence_list.append(confidence)
                face_list.append([a,b])
                #image = cv2.rectangle(image,a,b,(0,255,0),1)

        return face_list
        


def Manage_attribute(attr = None,typespec = None):
    def checkattribute(func):
        @wraps(func)    
        def wrapper(*args,**kwargs):
            for key,value in kwargs.items():
                print(key)
                if(attr in key):
                    if not isinstance(value,typespec):
                        raise TypeError("%s Attribute is not instance of %s",(attr,typespec))
            return func(*args,**kwargs)
        return wrapper
    return checkattribute


def custom_siamese(y_true,y_pred):
        margin = 1
        return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))

from xgboost import XGBClassifier                       
import matplotlib.pyplot as plt
def euclidean_distance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))


def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)
def draw_face_shape(image,a,b):
    x1,y1 = a
    x2,y2 =  b
    x ,y = int(abs(x1 + x2)/2) , int(abs(y1 + y2)/2)
    image2 = image.copy()
    cv2.ellipse(image2, (x,y), (int(abs(x1 - x2)/2),int(abs(y1 - y2)/2)), 0, 0, 360, (0,255,0), -1 )  
    image = cv2.addWeighted(image, 0.7, image2, 0.3, 0)
    #image  = cv2.circle(image,(x,y), 50, (0,0,255), -1)
    #cv2.ellipse(image, (x,y), (int(abs(x1 - x2)/2),int(abs(y1 - y2)/2)), 0, 0, 360, (0,0,255), -1 )
    return image




class Image_Augmentation:
    def __init__(self):
        self.final_images = []

    def Image_Crop_pad(self,image):
        image1 = cv2.resize(image[:104,:] , (160,160) , interpolation = cv2.INTER_AREA )
        image2 = cv2.resize(image[:,:104], (160,160) , interpolation = cv2.INTER_AREA )
        image3 = cv2.resize(image[56:,:], (160,160) , interpolation = cv2.INTER_AREA )
        image4 = cv2.resize(image[:,56:], (160,160) , interpolation = cv2.INTER_AREA )
        return image1 , image2 , image3 ,image4  

    def rotate_bound(self,image, angle):

        (h, w) = image.shape[:2]
        (cX, cY) = (w // 2, h // 2)

        M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)
        cos = np.abs(M[0, 0])
        sin = np.abs(M[0, 1])
        nW = int((h * sin) + (w * cos))
        nH = int((h * cos) + (w * sin))

        M[0, 2] += (nW / 2) - cX
        M[1, 2] += (nH / 2) - cY
        image = cv2.warpAffine(image, M, (nW, nH))
        return cv2.resize(image[10:-10,10:-10], (160,160) , interpolation = cv2.INTER_AREA )

    def gausssian_blur(self,image):
        return cv2.GaussianBlur(image,(3,3),cv2.BORDER_DEFAULT)

    def Build_Dataset(self,images ,labels):
        final_images = []
        final_labels = []
        for image , label in zip(images,labels):
            x,y,x1,y1 = self.Image_Crop_pad(image)
            image15 = self.rotate_bound(image,15)
            image_neg_15 = self.rotate_bound(image,-15)
            image_gaussian = self.gausssian_blur(image)
            final_images.append(x)
            final_labels.append(label)
            final_images.append(y)
            final_labels.append(label)
            final_images.append(x1)
            final_labels.append(label)
            final_images.append(y1)
            final_labels.append(label)
            final_images.append(image15)
            final_labels.append(label)
            final_images.append(image_neg_15)
            final_labels.append(label)
            final_images.append(image_gaussian)
            final_labels.append(label)
        print('====================final check' , len(final_images)  , len(final_labels))

        return final_images , final_labels


from sklearn.metrics.pairwise import cosine_similarity
def cosine_distance(x,y):
    return cosine_similarity([x],[y])


class Face_recognition(metaclass = metaclass_singleton ):
    def __init__(self,path:str,Neighbors_KNN=5):
        '''model = cv2.dnn.readNetFromCaffe('OpenCV_repo_data/deploy.prototxt', 'OpenCV_repo_data/weights.caffemodel')
        self.df = DetectFaceBoundBoxes(model)
        self.Neighbors_KNN = Neighbors_KNN'''
        self.model = cv2.dnn.readNetFromCaffe('OpenCV_repo_data/deploy.prototxt', 'OpenCV_repo_data/weights.caffemodel')
        self.df = DetectFaceBoundBoxes(self.model)
        self.Neighbors_KNN = Neighbors_KNN

        @typeassert(str)
        def check(path):
            self.path = path
            type(self) 
            self._result = []
            self.knownfaces = []
            self.label = []
            self.classes = np.load('class_embedding_average.npy')
            self._font = cv2.FONT_HERSHEY_COMPLEX_SMALL
            array = self.load_model(r'mean_std.pickle')
            self._mean , self._std  = array[0] , array[1]
            self._model = self.loadModel()
            self.modelsvc = self.load_model(r'svc.pickle')
            self._le = self.load_model(r'labelencoder.pickle')

            self._normalizer = self.load_model(r'normalizer.pickle')
        check(path)
    def __enter__(self):
        self.model = cv2.dnn.readNetFromCaffe('OpenCV_repo_data/deploy.prototxt', 'OpenCV_repo_data/weights.caffemodel')
        self.df = DetectFaceBoundBoxes(self.model)
        return self
    
    def __exit__(self , exc_type, exc_value, exc_traceback):
        del self.model
        pass
    @property
    def get_path(self):
        return self.path
    
    @get_path.setter
    def get_gath(self,path):
        if not isinstance(path,str):
            raise TypeError('String Expected')
    
    @get_path.deleter
    def get_gath(self):
        raise TypeError('String Expected')
        
    def save_model(self,file,name):
        with open(name, "wb") as output_file:
            pickle.dump(file, output_file)
            
    def load_model(self,name):
        with open(name, "rb") as input_file:
            return pickle.load(input_file)
    def loadModel(self):
        from keras.models import load_model
        model = load_model('FaceNet/facenet_keras.h5')
        return model
        

    def detectFace(self, img:list ,modelsvc) -> list:
        h , w = img.shape[:2]
        a,b,confidence = self.df.detectFace(img , h , w)
        if(a is not None ):
            x1,y1 = a
            x2,y2 = b
            if(x1 > 0 and  x2 > 0  and y1 > 0 and y2 > 0):
                

                tempimg = img
                croped_image = tempimg[y1:y2,x1:x2]
                if(len(croped_image) != 0 ):

                    tempimg =  cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)
                    #tempimg =  cv2.resize(tempimg, (160,160), interpolation = cv2.INTER_AREA)
                else:
                    tempimg =  cv2.resize(tempimg, (160,160), interpolation = cv2.INTER_AREA)

                tempimg = ( tempimg - self._mean ) / self._std

                _embeddings = self._model.predict(np.reshape(tempimg,(1,160,160,3)))
                _embeddings = self._normalizer.transform(_embeddings)
                result = modelsvc.predict(_embeddings)
                resultlabel = self._le.inverse_transform(result)
                img = cv2.putText(img,resultlabel[0],(x1,y1),self._font, 1,(0,255,0),1,cv2.LINE_AA)
                img = cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),1)
        return img
    
    def multi_detectFace(self,img ):
        h , w = img.shape[:2]
        face_list = self.df.detect_multiFace(img , h , w)
        final_tempimg = []
        for faces in face_list:
            if(len(faces) != 0 ):
                a = faces[0]
                b = faces[1]
                x1,y1 = a
                x2,y2 = b
                if(x1 > 0 and  x2 > 0  and y1 > 0 and y2 > 0):
                    tempimg = img
                    croped_image = tempimg[y1:y2,x1:x2]
                    if(len(croped_image) != 0 ):
                        tempimg =  cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)
                        #tempimg =  cv2.resize(tempimg, (160,160), interpolation = cv2.INTER_AREA)
                    else:
                        tempimg =  cv2.resize(tempimg, (160,160), interpolation = cv2.INTER_AREA)
                    tempimg = cv2.GaussianBlur(tempimg,(3,3),0)

                    tempimg = ( tempimg - self._mean ) / self._std

                    final_tempimg.append(tempimg)

        if(len(final_tempimg) != 0):

            final_tempimg = np.asarray(final_tempimg)
            _embeddings = self._model.predict(final_tempimg)
            _embeddings = self._normalizer.transform(_embeddings)
            #result = modelsvc.predict(_embeddings)
            confidence = self.modelsvc.predict_proba(_embeddings)

            #resultlabel = self._le.inverse_transform(result)
            '''for label,coor in zip(resultlabel , face_list):
                cv2.putText(img,label,coor[0],self._font, 2,(255,255,255),1,cv2.LINE_AA)
                cv2.rectangle(img,coor[0],coor[1],(0,255,0),1)'''
            print('confidence list' , confidence)
            print('########################################difference')
            max_distance = []
            for embed in _embeddings:
                
                for class_ in self.classes:
                    print(class_.shape , embed.shape)
                    max_distance.append(cosine_distance(class_,embed))


            print('max cosine_similarity is:' , np.max(max_distance))

            for conf,coor in zip(confidence , face_list):
                label = ''
                if(np.max(conf) < 0.75):
                    label = 'Unknown'
                else:
                    value = np.argmax(conf)
                    label = self._le.inverse_transform([value])
                    label = label[0] + ':' +str(int(np.max(conf)*100)) + '%'
                img2 = img.copy()
                x1,y1 = coor[0]
                x2,y2 = coor[1]
                
                cv2.rectangle(img2,(x1,y1-25),(x2,y1),(0,255,0),cv2.FILLED)
                cv2.rectangle(img,coor[0],coor[1],(0,255,0),1,cv2.LINE_8)
                img = cv2.addWeighted(img, 0.7, img2, 0.3, 0)
                cv2.putText(img,label,(x1,y1-5),self._font, 1,(250,0,0),1,cv2.LINE_8) 
                #img = draw_face_shape(img,coor[0],coor[1]) 
        return img

    
    def identifyFaces(self,img:list) :
        resultlabel = ""
        result,resulttupple = generate_faces_test([img])
        if(len(result)>0):
            embeddings = model.predict([result])
            xtest = ie.fit_transform(embeddings)
            result = modelsvc.predict(xtest)
            resultlabel = le.inverse_transform(result)
        return resultlabel,resulttupple
    
    def Identifyface_video(self,video_ = 0):
        cap = cv2.VideoCapture(video_)
        
        count = 0
        font = cv2.FONT_HERSHEY_SIMPLEX
        array = self.load_model(r'mean_std.pickle')
        self._mean , self._std  = array[0] , array[1]
        self._model = self.loadModel()
        modelsvc = self.load_model(r'svc.pickle')
        self._le = self.load_model(r'labelencoder.pickle')
        self._normalizer = self.load_model(r'normalizer.pickle')
        
        while(True):
            start_time = time.time()
            ret, frame = cap.read()
            frame = self.detectFace(frame,modelsvc)
            current_time = time.time() - start_time
            fps = 1 / current_time
            print(fps)
            frame = cv2.putText(frame,'FPS:' + str(math.ceil(fps)),(frame.shape[0]-20,50),self._font, 1,(0,255,255),2,cv2.LINE_AA)
            cv2.imshow('FaceRecognitation',frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()
    def multi_face(self ,image ):
        count = 0
        first = True
        fps_list = []
        fps_deque = collections.deque(maxlen=100)
        while(True):
            start_time = time.time()
            ret, frame = cap.read()
            frame = self.multi_detectFace(frame,modelsvc)
            '''result = face_model.detect_faces(frame)
           
            if(len(result)>0):
                for i in result:
                    temp = i.get('box')
                    x1, y1 = abs(temp[0]), abs(temp[1])
                    x2, y2 = x1 + temp[2], y1 + temp[3]
                    cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),1) '''
            current_time = time.time() - start_time
            fps = 1 / current_time
            fps = round(Decimal(fps),2)
            if (first is True):
                avg_fps = fps
                max_fps = fps
                min_fps = fps
                fps_deque.append(fps)
                first = False
            else:
                fps_deque.append(fps)
                fps_deque = sorted(fps_deque)
                avg_fps = round(Decimal(mean(fps_deque)),2)
                max_fps  = round(Decimal(fps_deque[0]),2)
                min_fps = round(Decimal(fps_deque[-1]),2)

            frame = cv2.putText(frame,'CURR_FPS:' + str(fps),(20,35),cv2.FONT_HERSHEY_PLAIN, 1,(0,0,255),1,cv2.LINE_AA)
            frame = cv2.putText(frame,'MEAN_FPS:' + str(avg_fps),(20,50),cv2.FONT_HERSHEY_PLAIN, 1,(0,0,255),1,cv2.LINE_AA)
            frame = cv2.putText(frame,'MIN_FPS:' + str(max_fps),(20,65),cv2.FONT_HERSHEY_PLAIN, 1,(0,0,255),1,cv2.LINE_AA)
            frame = cv2.putText(frame,'MAX_FPS:' + str(math.ceil(min_fps)),(20,80),cv2.FONT_HERSHEY_PLAIN, 1,(0,0,255),1,cv2.LINE_AA)

            cv2.imshow('FaceRecognitation',frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()


    def initialize_bias(self,shape, name=None):
    
        return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)

    def initialize_weights(self,shape, name=None):

        return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)
    

    def Base_net(self,input_shape):
        model = Sequential()
        print(input_shape)
        
        model.add(Dense(128, activation='relu',input_shape=(128,)))
        model.add(Dense(64,activation='relu'))
        model.add(Dense(64,activation='relu'))
        model.add(Dense(5,activation = 'relu'))
        return model
    def Conv_Base(self,input_shape):
        model = Sequential()
        initialize_weights  = self.initialize_weights
        initialize_bias = self.initialize_bias
        model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,
                       kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))
        model.add(MaxPooling2D())
        model.add(Conv2D(128, (7,7), activation='relu',
                         kernel_initializer=initialize_weights,
                         bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
        model.add(MaxPooling2D())
        model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,
                         bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
        model.add(MaxPooling2D())
        model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,
                         bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))
        model.add(Flatten())
        model.add(Dense(250, activation='sigmoid',
                       kernel_regularizer=l2(1e-3),
                       kernel_initializer=initialize_weights,bias_initializer=initialize_bias))
        return model

    def Siamese_net(self,feature_a,feature_b):
        base_model = self.Conv_Base((160,160,3))

        out_a =  base_model(feature_a)
        out_b =  base_model(feature_b)

        distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([out_a, out_b])

        model = Model(inputs = [feature_a,feature_b] , outputs = distance )
        adam = optimizers.Adam(lr = 0.0001)
        model.compile(loss=custom_siamese , optimizer = adam)
        return model
    def Siamese_Train(self,x1,x2,y):
        img_a = Input(shape= (160,160,3))
        img_b = Input(shape= (160,160,3))
        siamese_model = self.Siamese_net(img_a,img_b)
        siamese_model.summary()
        siamese_model.fit([x1,x2],y,epochs=100,validation_split = 0.1,verbose=1)
        return siamese_model


    
    @typeassert( str)
    def TrainFaces(self,train_path:str):

        for o, directory, files in os.walk(train_path):
            for filename in files:
                print('Loading image' + o+ '/'+ filename )
                temp = cv2.imread(o+ '/'+ filename)

                self.knownfaces.append([cv2.cvtColor(temp,cv2.COLOR_BGR2RGB),filename])
                self.label.append(o.split('/')[-1])
        #result  = generate_faces(knownfaces = self.knownfaces )
        result = []

        for key,i in  enumerate(self.knownfaces):
            print(i[1])
            i = i[0]
            h ,w   = i.shape[:2]
            print('######################################################################',h,w)
            a,b,confidence = self.df.detectFace(i,h,w)
            if(a is not None and b is not None):
                x1 , y1 = a
                x2 , y2 = b
                croped_image = i[y1:y2,x1:x2]
                if(len(croped_image)!=0):
                    result.append(cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA))
                else:
                    result.append(cv2.resize(i, (160,160), interpolation = cv2.INTER_AREA))

            else:
                del self.label[key]

        result = np.asarray(result)
        result = result.astype('float32')
        print('final check',len(result),len(self.label))


        

        self._mean, self._std = result.mean(), result.std()
        self.save_model(np.array([self._mean,self._std]),'mean_std.pickle')
        result = (result - self._mean) / self._std
        self._result = result
        model = self.loadModel()
        self._model = model
        _embeddings = model.predict(result)
        self._normalizer  = Normalizer(norm = 'l2')
        _embeddings = self._normalizer.fit_transform(_embeddings)
        self.save_model(self._normalizer,r'normalizer.pickle')
        self._le = LabelEncoder()
        encodedy = self._le.fit_transform(self.label)
        self.save_model(self._le,r'labelencoder.pickle')
        modelKNN =  KNeighborsClassifier(n_neighbors=5)
        modelKNN.fit(_embeddings,encodedy)
        print('Training Done! Saving model.')
        self.save_model(modelKNN,r'svc.pickle')

    def TrainFaces_SVM(self,train_path:str):
        ia = Image_Augmentation()
        for o, directory, files in os.walk(train_path):
            for filename in files:
                print('Loading image' + o+ '/'+ filename )
                temp = cv2.imread(o+ '/'+ filename)
                image_color = cv2.cvtColor(temp,cv2.COLOR_BGR2RGB)
                self.knownfaces.append([image_color,filename])
                self.label.append(o.split('/')[-1])
        #result  = generate_faces(knownfaces = self.knownfaces )
        result = []

        for key,i in  enumerate(self.knownfaces):
            print(i[1])
            i = i[0]
            h ,w   = i.shape[:2]
            print('######################################################################',h,w)
            a,b,confidence = self.df.detectFace(i,h,w)
            if(a is not None and b is not None):
                x1 , y1 = a
                x2 , y2 = b
                croped_image = i[y1:y2,x1:x2]
                if(len(croped_image)!=0):
                    result.append(cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA))
                else:
                    result.append(cv2.resize(i, (160,160), interpolation = cv2.INTER_AREA))

            else:
                del self.label[key]

        result = np.asarray(result)
        result = result.astype('float32')
        print('final check',len(result),len(self.label))






        '''

        IMAGE AUGMENTATION FOR BETTER RESULTS

        '''
        ia = Image_Augmentation()
        final_x ,final_y = ia.Build_Dataset(result,self.label)
        result = np.asarray(final_x)
        self.label = np.asarray(final_y)


        self._mean, self._std = result.mean(), result.std()
        self.save_model(np.array([self._mean,self._std]),'mean_std.pickle')
        result = (result - self._mean) / self._std
        self._result = result
        model = self.loadModel()
        self._model = model
        _embeddings = model.predict(result)
        self._normalizer  = Normalizer(norm = 'l2')

        _embeddings = self._normalizer.fit_transform(_embeddings)




        classes = np.unique(self.label)
        class_means = []
        for i in classes:
            class_ = []
            for embed,label in zip(_embeddings,self.label):
                if(i == label):
                    class_.append(embed)

            class_ = np.asarray(class_)
            print(class_.shape)
            mean = np.mean(class_ , axis= 0)
            class_means.append(mean)
            print(mean.shape)

        print(len(class_means))
        class_means = np.asarray(class_means)
        np.save('class_embedding_average',class_means)


        self.save_model(self._normalizer,r'normalizer.pickle')
        self._le = LabelEncoder()
        encodedy = self._le.fit_transform(self.label)
        self.save_model(self._le,r'labelencoder.pickle')
        svc =  SVC(C=1)
        #'C': 10, 'gamma': 0.001, 'kernel': 'linear'}
        parameters = {'C':[0.1,0.001,10,20,40]  ,'kernel':['poly' , 'linear' ,'rbf' ],  'gamma':[0.001 , 0.01,0, 1,20,30,40,50, 10, 100]}
        clf = GridSearchCV(svc, parameters, cv=15)
        clf.fit(_embeddings,encodedy)
        print(clf.best_params_)
        #return
        svc = SVC(**clf.best_params_ ,probability=True)
        svc.fit(_embeddings,encodedy)
        print('Training Done! Saving model.')
        self.save_model(svc,r'svc.pickle')


    def TrainFaces_XGBoost(self,train_path:str):
            for o, directory, files in os.walk(train_path):
                for filename in files:
                    print('Loading image' + o+ '/'+ filename )
                    temp = cv2.imread(o+ '/'+ filename)
                    self.knownfaces.append([cv2.cvtColor(temp,cv2.COLOR_BGR2RGB),filename])
                    self.label.append(o.split('/')[-1])
            #result  = generate_faces(knownfaces = self.knownfaces )
            result = []

            for key,i in  enumerate(self.knownfaces):
                print(i[1])
                i = i[0]
                h ,w   = i.shape[:2]
                print('######################################################################',h,w)
                a,b,confidence = self.df.detectFace(i,h,w)
                if(a is not None and b is not None):
                    x1 , y1 = a
                    x2 , y2 = b
                    croped_image = i[y1:y2,x1:x2]
                    if(len(croped_image)!=0):
                        result.append(cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA))
                        #result.append(cv2.resize(i, (160,160), interpolation = cv2.INTER_AREA))
                    else:
                        result.append(cv2.resize(i, (160,160), interpolation = cv2.INTER_AREA))

                else:
                    del self.label[key]

            result = np.asarray(result)
            result = result.astype('float32')
            print('final check',len(result),len(self.label))
            self._mean, self._std = result.mean(), result.std()
            self.save_model(np.array([self._mean,self._std]),'mean_std.pickle')
            result = (result - self._mean) / self._std
            self._result = result
            model = self.loadModel()
            self._model = model
            _embeddings = model.predict(result)
            self._normalizer  = Normalizer(norm = 'l2')
            _embeddings = self._normalizer.fit_transform(_embeddings)
            self.save_model(self._normalizer,r'normalizer.pickle')
            self._le = LabelEncoder()
            encodedy = self._le.fit_transform(self.label)
            self.save_model(self._le,r'labelencoder.pickle')
            svc = XGBClassifier()
            #svc =  SVC(kernel = 'linear')
            svc.fit(_embeddings,encodedy)
            print('Training Done! Saving model.')
            self.save_model(svc,r'svc.pickle')


    def TestModel(self , test_path):
        count = 0
        array = self.load_model(r'mean_std.pickle')
        self._mean , self._std  = array[0] , array[1]
        self._model = self.loadModel()
        modelsvc = self.load_model(r'svc.pickle')
        self._le = self.load_model(r'labelencoder.pickle')
        self._normalizer = self.load_model(r'normalizer.pickle')
        total = 0
        for test_image in listdir(test_path+'/'):
            total += 1
            img = cv2.imread(test_path+'/'+test_image)
            _,label = self.detectFace(img,modelsvc)
            print('label for image: '+str(test_image)+'is '+ label)
            print(label)
            if(label in str(test_image)[:-4]):
                count += 1

        print('accuracy is ....' , count/total)



class Prepare_Dataset_Siamese:
    def __init__(self,path):
        self.path = path
        self.xtrain_pos = []
        self.xtrain_neg = []
        model = cv2.dnn.readNetFromCaffe('OpenCV_repo_data/deploy.prototxt', 'OpenCV_repo_data/weights.caffemodel')
        self.df = DetectFaceBoundBoxes(model)
        
        fr = Face_recognition('test')
        self.model  = fr.loadModel()

    def Form_Pairs(self,pos,neg):
        inputs = [pos,neg]
        pos_pairs = [list(i) for i in itertools.combinations(pos , 2)]
        random.shuffle(pos_pairs)
        random.shuffle(pos_pairs)
        pos_pairs = pos_pairs

        neg_pairs = [list(i) for i in itertools.product(*inputs)]
        random.shuffle(neg_pairs)
        random.shuffle(neg_pairs)
        neg_pairs = neg_pairs
        pos_label = np.ones((len(pos_pairs),1))
        neg_label = np.zeros((len(neg_pairs),1))
        
        new_pos = []
        for i in pos_pairs:
            new_pos.append([i,0])
        print(np.asarray(new_pos).shape)
        new_neg = []
        for i in neg_pairs:
            new_neg.append([i,1])

        
        xtrain = np.concatenate((np.asarray(new_pos) , np.asarray(new_neg) ), axis = 0)
        return xtrain
    def Load_Train_Dataset_image(self):
        first = True
        xtrain = []
        for i in listdir(self.path):
            for directory in listdir(self.path+i+'/'):
                if('positive' in directory):
                    for images in listdir(self.path+i+'/positive/'):
                        print(self.path+i+'/positive/'+images)
                        img = cv2.imread((self.path+i+'/positive/'+images))
                        
                        a,b,confidence = self.df.detectFace(img,img.shape[0],img.shape[1])

                        if(a is not None and b is not None):
                            x1 , y1 = a
                            x2 , y2 = b
                            croped_image = img[y1:y2,x1:x2]

                            if(len(croped_image)!=0):

                                self.xtrain_pos.append([cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)] )
                            else:
                                self.xtrain_pos.append([cv2.resize(img, (160,160), interpolation = cv2.INTER_AREA)])
                else:
                    for images in listdir(self.path+i+'/negative/'):
                        print(self.path+i+'/negative/'+images)
                        img = cv2.imread(self.path+i+'/negative/'+images)
                        a,b,confidence = self.df.detectFace(img,img.shape[0],img.shape[1])
                        
                        if(a is not None and b is not None):
                            x1 , y1 = a
                            x2 , y2 = b
                            croped_image = img[y1:y2,x1:x2]
                            if(len(croped_image)!=0):
                                self.xtrain_neg.append([cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)] )
                            else:
                                self.xtrain_neg.append([cv2.resize(img, (160,160), interpolation = cv2.INTER_AREA)])
            xtrain_pairs = self.Form_Pairs(self.xtrain_pos , self.xtrain_neg)
            if(first==True):
                xtrain = list(xtrain_pairs)
                first = False
            else:
                xtrain+=list(xtrain_pairs)

        return xtrain 

    def Load_Train_Dataset(self ):
        first = True
        xtrain = []
        for i in listdir(self.path):
            for directory in listdir(self.path+i+'/'):
                if('positive' in directory):
                    for images in listdir(self.path+i+'/positive/'):
                        print(self.path+i+'/positive/'+images)
                        img = cv2.imread((self.path+i+'/positive/'+images))
                        
                        a,b,confidence = self.df.detectFace(img,img.shape[0],img.shape[1])

                        if(a is not None and b is not None):
                            x1 , y1 = a
                            x2 , y2 = b
                            croped_image = img[y1:y2,x1:x2]

                            if(len(croped_image)!=0):

                                self.xtrain_pos.append([cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)] )
                            else:
                                self.xtrain_pos.append([cv2.resize(img, (160,160), interpolation = cv2.INTER_AREA)])
                else:
                    for images in listdir(self.path+i+'/negative/'):
                        print(self.path+i+'/negative/'+images)
                        img = cv2.imread(self.path+i+'/negative/'+images)
                        a,b,confidence = self.df.detectFace(img,img.shape[0],img.shape[1])
                        
                        if(a is not None and b is not None):
                            x1 , y1 = a
                            x2 , y2 = b
                            croped_image = img[y1:y2,x1:x2]
                            if(len(croped_image)!=0):
                                self.xtrain_neg.append([cv2.resize(croped_image, (160,160), interpolation = cv2.INTER_AREA)] )
                            else:
                                self.xtrain_neg.append([cv2.resize(img, (160,160), interpolation = cv2.INTER_AREA)])
            xtrain_pos = np.asarray(self.xtrain_pos)
            xtrain_neg = np.asarray(self.xtrain_neg)
            xtrain_pos = np.reshape(xtrain_pos,(xtrain_pos.shape[0],160,160,3))
            xtrain_neg = np.reshape(xtrain_neg,(xtrain_neg.shape[0],160,160,3))


            xtrain_pos = self.model.predict(xtrain_pos)
            xtrain_neg = self.model.predict(xtrain_neg)
            xtrain_pairs = self.Form_Pairs(xtrain_pos , xtrain_neg)
            if(first==True):
                xtrain = list(xtrain_pairs)
                first = False
            else:
                xtrain+=list(xtrain_pairs)

        return xtrain 
            



                    


"""# context manager for detecting realtime faces in video"""

'''with Face_recognition("Testvideo/test.mp4") as fr:
    
    #fr.TrainFaces_SVM('Known_Faces/') #FOR TRAINING ON CUSTOM DATASET SPECIF
    #fr.TestModel('know_faces_test/test')
    #fr.Siamese_Train()
    fr.multi_face(video_ = 0)'''



'''

TRAIN SIAMESE NETWORK
'''


'''p = Prepare_Dataset_Siamese('Known_Faces/')
xtrain  = p.Load_Train_Dataset_image()
xtrain  = np.asarray(xtrain)

xtrain_img1 = []
xtrain_img2 = []
y = np.asarray(xtrain[:,1])
for i in xtrain[:,0]:
    xtrain_img1.append(i[0])
    xtrain_img2.append(i[1])
print(np.asarray(xtrain_img1).shape)
xtrain_img1 = np.asarray(xtrain_img1)
xtrain_img2 = np.asarray(xtrain_img2)
fr = Face_recognition("Testvideo/test.mp4")
fr.Siamese_Train(np.reshape(np.asarray(xtrain_img1) , (xtrain_img1.shape[0],160,160,3)),np.reshape(np.asarray(xtrain_img1) , (xtrain_img2.shape[0],160,160,3)) , y)
'''
























